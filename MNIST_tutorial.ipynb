{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "# MNIST Large Data/ Many Batches Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The purpose of this tutorial is to examine a sample application that was created using the [Intel® Distribution of Open Visual Inference & Neural Network Optimization (OpenVINO™) toolkit](https://software.intel.com/openvino-toolkit).  This tutorial will go step-by-step through the necessary steps to demonstrate object classification on images and batches of images.  Classification is performed using a pre-trained network and running it using the Intel® Distribution of OpenVINO™ toolkit Inference Engine.  Inference will be executed using the same CPU(s) running this Jupyter* Notebook.\n",
    "\n",
    "The pre-trained model to be used for object detection is a custom, basic TensorFlow model which has already been converted to the necessary Intermediate Representation (IR) files needed by the Inference Engine (Conversion is not covered here, please see the [Intel® Distribution of OpenVINO™ toolkit](https://software.intel.com/en-us/openvino-toolkit) documentation for more details).  The model is capable of classifying the images in the MNIST dataset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "This sample requires the following:\n",
    "- All files are present and in the following directory structure:\n",
    "    - **1000mnist32.xml** = The .xml IR file\n",
    "    - **1000mnist32.bin** = the .bin IR file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Concepts\n",
    "\n",
    "The following sections will guide you through a model that is trained on the MNIST dataset (see http://yann.lecun.com/exdb/mnist/). It will also introduce the following concepts:\n",
    "\n",
    "<b>1: Using a large dataset</b>. Every other tutorial runs inference on a small amount of pictures, between 1-15. We will be using 10,000.\n",
    "\n",
    "<b>2: Multiple batches:</b> With a dataset of 10,000 images, we can't squeeze them all through in one batch without losing significant performance. Thus, we will be splitting it up into 10 batches of 1000 images each, a much more managable load.\n",
    "\n",
    "<b>3: Keeping track of a top-n accuracy. </b> For example: for n = 3, if the highest-weighted prediction by the model is not correct, we will also look at the second and third weighted predictions, and determine another accuracy based on that. We will numerically count the exact amount of images that were correctly inferenced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "We begin by importing all of the Python* modules that will be used by the sample code: These include\n",
    "- [os](https://docs.python.org/3/library/os.html#module-os) - Operating system specific module (used for file name parsing)\n",
    "- [cv2](https://docs.opencv.org/trunk/) - OpenCV module\n",
    "- [time](https://docs.python.org/3/library/time.html#module-time) - time tracking module (used for measuring execution time)\n",
    "- [numpy](http://www.numpy.org/) - n-dimensional array manipulation\n",
    "- [openvino.inference_engine](https://software.intel.com/en-us/articles/OpenVINO-InferEngine) - the IENetwork and IECore objects\n",
    "\n",
    "Run the cell below to import Python dependencies needed for displaying the results in this notebook. \n",
    "\n",
    "<br><div class=tip><b>Tip: </b>Select a cell and then use **Ctrl+Enter** to run that cell.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging as log\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import sys\n",
    "from argparse import ArgumentParser\n",
    "import applicationMetricWriter\n",
    "from time import time\n",
    "\n",
    "#Setup inference engine\n",
    "try:\n",
    "    from openvino import inference_engine as ie\n",
    "    from openvino.inference_engine import IENetwork, IECore, IEPlugin\n",
    "    \n",
    "except Exception as e:\n",
    "    exception_type = type(e).__name__\n",
    "    print(\"The following error happened while importing Python API module:\\n[ {} ] {}\".format(exception_type, e))\n",
    "    sys.exit(1)\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the Large Dataset\n",
    "Here we will create and set the dataset to use. We will be importing in the entire dataset from TensorFlow (see https://www.tensorflow.org/datasets/catalog/mnist_, for demonstration purposes (if you wanted, you could set it up to run inference on all 70,000 images-- though that wouldn't be very informative, because it was trained on the first 60,000). You can change the amount of images you want to run inference on, but we will simply be running it on the final 10,000 images in the dataset. We will parition the dataset to get the  images to use.\n",
    "\n",
    "This process saves you from downloading all those images manually, and uploading them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the Dataset\n",
    "import tensorflow as tf\n",
    "\n",
    "#  Load and normalize Dataset\n",
    "(void1, void2), (test_images, test_labels) = tf.keras.datasets.mnist.load_data() \n",
    "\n",
    "test_images =  test_images[..., np.newaxis]/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "Here we will create the following configuration parameters to be used by the sample. The following three are the most important, and the others can be seen in the following block of code.\n",
    "\n",
    "* **-m, model** - Path to the .xml IR file of the trained model to use for inference.\n",
    "* **-b, batch_size** - The batch size to use. This is a variable amount depending on your model, and will be explained later in this tutorial.\n",
    "* **-d, device** - Specify the target device to infer on,  CPU, GPU, FPGA, or MYRIAD is acceptable, however the device must be present.  For this tutorial we use \"CPU\" which is known to be present.\n",
    "\n",
    "Note that, unlike other tutorials, we do not need to specify a -i parameter. This is because the necessary input data is imported directly into the code, and so we do not need to provide a file path to the images (or even the labels).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration parameters settings:\n",
      "\tmodel_xml= ./models/1000mnist32.xml \n",
      "\tmodel_bin= ./models/1000mnist32.bin \n",
      "\tdevice= CPU\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "\n",
    "#Change the batch size to the model you're using\n",
    "batch_size = 1000\n",
    "\n",
    "# keep these files in the same directory\n",
    "model_xml = \"./models/\" + str(batch_size) + \"mnist32.xml\"\n",
    "model_bin = os.path.splitext(model_xml)[0] + \".bin\"\n",
    "\n",
    "# device to use\n",
    "device = \"CPU\"\n",
    "\n",
    "print(\"Configuration parameters settings:\"\n",
    "     \"\\n\\tmodel_xml=\", model_xml,\n",
    "      \"\\n\\tmodel_bin=\", model_bin,\n",
    "       \"\\n\\tdevice=\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create inference engine instance\n",
    "\n",
    "Next we create the Inference Engine instance to be used by our application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plugin initialization for specified device and load extensions library if specified\n",
    "log.info(\"Initializing plugin for {} device...\".format(device))\n",
    "ie = IECore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create network\n",
    "\n",
    "Here we create an IENetwork object and load the model's IR files into it. After loading the model, we check to make sure that all the model's layers are supported by the plugin we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model to the plugin\n",
      "Successfully loaded\n"
     ]
    }
   ],
   "source": [
    "# Load network from IR files\n",
    "log.info(\"Reading IR...\")\n",
    "net = ie.read_network(model=model_xml, weights=model_bin)\n",
    "\n",
    "# Check that model layers are supported\n",
    "if device == \"CPU\":\n",
    "    supported_layers = ie.query_network(net, \"CPU\")\n",
    "    not_supported_layers = [l for l in net.layers.keys() if l not in supported_layers]\n",
    "    if len(not_supported_layers) != 0:\n",
    "        log.warning(\"Following layers are not supported by the plugin for specified device {}:\\n {}\".\n",
    "                  format(args.device, ', '.join(not_supported_layers)))\n",
    "        log.warning(\"Please try to specify cpu extensions library path in sample's command line parameters using -l \"\n",
    "                  \"or --cpu_extension command line argument\")\n",
    "        sys.exit(1)\n",
    "\n",
    "# Load network to the plugin\n",
    "print(\"Loading model to the plugin\")\n",
    "exec_net = ie.load_network(network=net, device_name=device)\n",
    "print(\"Successfully loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the batch size\n",
    "\n",
    "  After loading, we store the names of the input (`input_blob`) and output (`output_blob`) blobs to use when accessing the input and output blobs of the model.  Lastly, we store the batch size as \"x\" for easier use throughout:\n",
    "- `x` = The inputted batch size (In this case, was set to 1,000. Since the dataset has 10,000 images, we will run inference on 10 batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing input blobs\n",
      "Batch size is 1000\n",
      "\n",
      "Ready to move on!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: 'inputs' property of IENetwork class is deprecated. To access DataPtrs user need to use 'input_data' property of InputInfoPtr objects which can be accessed by 'input_info' property.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing input blobs\")\n",
    "\n",
    "input_blob = next(iter(net.inputs))\n",
    "out_blob = next(iter(net.outputs))\n",
    "\n",
    "#We define the batch size as x for easier use throughout\n",
    "x = batch_size\n",
    "print(\"Batch size is {}\".format(x))\n",
    "\n",
    "print(\"\\nReady to move on!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define multi-batch function\n",
    "\n",
    "Here, we define a function that is the essence of running multiple batches. We will treat each batch as a separate inference object, and iterate through the dataset in the specified amount of batches. This function, run_it, runs the inference on each batch of 1,000 images. The following variables are used in the function:\n",
    "- `correct` = The amount of images the model accurately infers\n",
    "- `wrong` = The amount of images the model infers incorrectly\n",
    "- `total_inference` = A global counter for inference time, added to on every batch\n",
    "- `j` = A global counter for indexing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for counting accuracy\n",
    "correct = 0\n",
    "wrong = 0\n",
    "total_inference = 0\n",
    "\n",
    "# A global counter (for simplification)\n",
    "j = 0\n",
    "\n",
    "#Function that will run multiple batches\n",
    "def run_it(start):\n",
    "    #Setup an array to run inference on (of the correct batch size)\n",
    "    pics = np.ndarray(shape=(x, 1, 28, 28))\n",
    "\n",
    "    #Fill up the input array\n",
    "    #setting up the end bound (exclusive)\n",
    "    stop = start + x \n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for item in test_images[start:stop]:\n",
    "        pics[i] = item.transpose(2,0,1)\n",
    "        i += 1\n",
    "\n",
    "    # Loading model to the plugin    \n",
    "    # Start inference\n",
    "    infer_time = []\n",
    "\n",
    "    t0 = time()\n",
    "    res = exec_net.infer(inputs={input_blob: pics})\n",
    "    infer_time.append((time()-t0)*1000)\n",
    "\n",
    "    # Processing output blob\n",
    "    res = res[out_blob]\n",
    "\n",
    "    global correct\n",
    "    global wrong\n",
    "    global j\n",
    "\n",
    "    # Accuracy counters\n",
    "    for i, probs in enumerate(res):\n",
    "        probs = np.squeeze(probs)\n",
    "        \n",
    "        # Top 5 results stored in top_ind\n",
    "        top_ind = np.argsort(probs)[-5:][::-1]\n",
    "        det_label = top_ind[0]\n",
    "\n",
    "        if det_label == test_labels[j]:\n",
    "            correct = correct + 1\n",
    "        else:\n",
    "            wrong = wrong + 1        \n",
    "\n",
    "        j = j + 1        \n",
    "\n",
    "    global total_inference\n",
    "    total_inference += np.sum(np.asarray(infer_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run inference\n",
    "\n",
    "Now, we can setup the amount of images to run on by calculating the amount of batches. We will iterate through the dataset, running inference on each successive batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference: Batch 1\n",
      "Running inference: Batch 2\n",
      "Running inference: Batch 4\n",
      "Running inference: Batch 6\n",
      "Running inference: Batch 8\n",
      "Running inference: Batch 10\n"
     ]
    }
   ],
   "source": [
    "#Iterate through the whole dataset\n",
    "num_batches = test_images.shape[0]//x\n",
    "\n",
    "# Set a variable for the loop (will increment by batch_size)\n",
    "k = 0\n",
    "\n",
    "# Ensure the global variables are default\n",
    "j = 0\n",
    "correct = 0\n",
    "wrong = 0\n",
    "total_inference = 0\n",
    "\n",
    "print(\"Running inference: Batch 1\")\n",
    "#Run it on all the batches\n",
    "for i in range(num_batches):\n",
    "    if (i + 1) % 2 == 0:\n",
    "        print(\"Running inference: Batch \" + str(i + 1))\n",
    "    run_it(k)\n",
    "    k += x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process and display results\n",
    "\n",
    "Now we display the inference results by printing out some math with the global counters we measured during the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct 9873\n",
      "Wrong 127\n",
      "Accuracy: 0.9873\n",
      "Average running time of one batch: 14.571356773376465 ms\n",
      "Total running time of inference: 145.71356773376465 ms\n",
      "Throughput: 68627.78913128491 FPS\n"
     ]
    }
   ],
   "source": [
    "# Print results    \n",
    "print(\"Correct \" + str(correct))\n",
    "print(\"Wrong \" + str(wrong))\n",
    "print(\"Accuracy: \" + str(correct/(correct + wrong)))\n",
    "\n",
    "print(\"Average running time of one batch: {} ms\".format(total_inference/num_batches))\n",
    "print(\"Total running time of inference: {} ms\" .format(total_inference))\n",
    "print(\"Throughput: {} FPS\".format((1000*x*num_batches)/total_inference))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise #1: Display the top <i>n</i> results\n",
    "\n",
    "We can actually create another function to display whether the correct answer is in the top <i>n</i> inferences. For this specific tutorial, it doesn't add very much, but it may be very useful in your own projects where models are likely to have a lower succesful inference rate.\n",
    "\n",
    "We define another global counter, top_n, to keep track of this value. We also write a function to iterate through the top <i>n</i> predictions, instead of just the top 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for counting accuracy\n",
    "correct = 0\n",
    "wrong = 0\n",
    "total_inference = 0\n",
    "\n",
    "# A global counter (for simplification)\n",
    "j = 0\n",
    "\n",
    "# A global counter for top_n correct\n",
    "top_n = 0\n",
    "\n",
    "#Function that will run multiple batches\n",
    "\n",
    "#Add a parameter n for the top-n results\n",
    "def run_it(start, n):\n",
    "    #Setup an array to run inference on (of the correct batch size)\n",
    "    pics = np.ndarray(shape=(x, 1, 28, 28))\n",
    "\n",
    "    #Fill up the input array\n",
    "    #setting up the end bound (exclusive)\n",
    "    stop = start + x \n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for item in test_images[start:stop]:\n",
    "        pics[i] = item.transpose(2,0,1)\n",
    "        i += 1\n",
    "\n",
    "    # Loading model to the plugin    \n",
    "    # Start inference\n",
    "    infer_time = []\n",
    "\n",
    "    t0 = time()\n",
    "    res = exec_net.infer(inputs={input_blob: pics})\n",
    "    infer_time.append((time()-t0)*1000)\n",
    "\n",
    "    # Processing output blob\n",
    "    res = res[out_blob]\n",
    "\n",
    "    global correct\n",
    "    global wrong\n",
    "    global j\n",
    "    global top_n\n",
    "    \n",
    "    # NEW FUNCTION to keep track of the top_n correct\n",
    "    def top_n_accuracy(n):\n",
    "        global top_n\n",
    "        \n",
    "        for i in range(n):\n",
    "            det_label = top_ind[i]\n",
    "\n",
    "            if det_label == test_labels[j]:\n",
    "                top_n = top_n + 1\n",
    "                return\n",
    "\n",
    "    # Accuracy counters\n",
    "    for i, probs in enumerate(res):\n",
    "        probs = np.squeeze(probs)\n",
    "        \n",
    "        # Top 5 results stored in top_ind\n",
    "        top_ind = np.argsort(probs)[-10:][::-1]\n",
    "        det_label = top_ind[0]\n",
    "\n",
    "        if det_label == test_labels[j]:\n",
    "            correct = correct + 1\n",
    "        else:\n",
    "            wrong = wrong + 1        \n",
    "\n",
    "        # Run our function\n",
    "        top_n_accuracy(n)\n",
    "        \n",
    "        j = j + 1        \n",
    "        \n",
    "    global total_inference\n",
    "    total_inference += np.sum(np.asarray(infer_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can play around with the variable <i>n</i> to determine the amount we want to print. It is currently at 3, with which the model is able to predict at 0.9995 accuracy. We can play around with running these two boxes back and forth, and find that it takes until n = 6 for the model to have 100% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference: Batch 1\n",
      "Running inference: Batch 2\n",
      "Running inference: Batch 4\n",
      "Running inference: Batch 6\n",
      "Running inference: Batch 8\n",
      "Running inference: Batch 10\n"
     ]
    }
   ],
   "source": [
    "#Iterate through the whole dataset\n",
    "num_batches = test_images.shape[0]//x\n",
    "\n",
    "# Set a variable for the loop (will increment by batch_size)\n",
    "k = 0\n",
    "\n",
    "# Ensure the global variables are default\n",
    "j = 0\n",
    "correct = 0\n",
    "wrong = 0\n",
    "total_inference = 0\n",
    "top_n = 0\n",
    "\n",
    "#Change this to the amount of values you want to test\n",
    "n = 3\n",
    "\n",
    "print(\"Running inference: Batch 1\")\n",
    "#Run it on all the batches\n",
    "for i in range(num_batches):\n",
    "    if (i + 1) % 2 == 0:\n",
    "        print(\"Running inference: Batch \" + str(i + 1))\n",
    "    run_it(k, n)\n",
    "    k += x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct 9873\n",
      "Wrong 127\n",
      "Accuracy: 0.9873\n",
      "Top 3 Correct: 9995\n",
      "Top 3 Accuracy: 0.9995\n",
      "\n",
      "Average running time of one batch: 7.371711730957031 ms\n",
      "Total running time of inference: 73.71711730957031 ms\n",
      "Throughput: 135653.70384744753 FPS\n"
     ]
    }
   ],
   "source": [
    "# Print results    \n",
    "print(\"Correct \" + str(correct))\n",
    "print(\"Wrong \" + str(wrong))\n",
    "print(\"Accuracy: \" + str(correct/(correct + wrong)))\n",
    "print(\"Top \" + str(n) + \" Correct: \" + str(top_n))\n",
    "print(\"Top \" + str(n) + \" Accuracy: \" + str(top_n/(correct + wrong)))\n",
    "    \n",
    "print(\"\")\n",
    "\n",
    "print(\"Average running time of one batch: {} ms\".format(total_inference/num_batches))\n",
    "print(\"Total running time of inference: {} ms\" .format(total_inference))\n",
    "print(\"Throughput: {} FPS\".format((1000*x*num_batches)/total_inference))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise #2: Different sized batches\n",
    "\n",
    "This tutorial is done with a batch size of 1000, because when the model was generated with IR, it was given a manual input shape of [1000, 24, 24, 1] (this shape referring to the 1-layer depth of the 24x24 MNIST images).\n",
    "\n",
    "The directory also contains models with base names:\n",
    "\n",
    "`500mnist32`, \n",
    "`2500mnist32`, and \n",
    "`10000mnist32`\n",
    "\n",
    "where the number before each one refers to the batch size the model is built for. You can run through this notebook again, changing the model parameters to the .xml and .bin versions of these files. You may observe that, while the throughput is similar for the models with batch size 500 and 2500, it decreases for the 10000. This is because squeezing 10,000 images through the inferencer at once is difficult, and so we split it up into multiple batches to increase throughtput. With larger datasets, the problem is only expounded. Of course, optimizing the model to find the batch size with the highest performance is the best thing to do in many scenarios, and our chosen size of 1000 is both near the peak throughput as well as an easy number to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report performance counters\n",
    "After running inference, the performance counters may be read from an internal request object using the function `get_perf_counts()` to see which layers of the inference model were run and how much time was spent in each.  Performance counts (metrics) reported include:\n",
    "- **name** - Name of layer within the inference model\n",
    "- **layer_type** - Type (or function) of layer (e.g. convolution, concat, etc.)\n",
    "- **exec_type** - Execution type for the layer.  The name may be used to identify which device has been run.  For example, entries starting with `jit_` indicate the CPU was used.\n",
    "- **status** - Whether the layer had been executed or not\n",
    "- **real_time** - Time in microseconds spent running layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance counters:\n",
      "name                                     layer_type      exec_type       status          real_time, us\n",
      "out_sequential/dense_1/BiasAdd/Add       Output          unknown_FP32    NOT_RUN         0         \n",
      "sequential/conv2d/BiasAdd/Add            Convolution     jit_avx512_FP32 EXECUTED        1622      \n",
      "sequential/conv2d/Relu                   ReLU            undef           NOT_RUN         0         \n",
      "sequential/conv2d_1/BiasAdd/Add          Convolution     jit_avx512_FP32 EXECUTED        2153      \n",
      "sequential/conv2d_1/Relu                 ReLU            undef           NOT_RUN         0         \n",
      "sequential/conv2d_2/BiasAdd/Add          Convolution     jit_avx512_FP32 EXECUTED        626       \n",
      "sequential/conv2d_2/Relu                 ReLU            undef           NOT_RUN         0         \n",
      "sequential/conv2d_2/Relu/Transpose       Permute         unknown_FP32    EXECUTED        142       \n",
      "sequential/dense/BiasAdd/Add             FullyConnected  jit_gemm_FP32   EXECUTED        668       \n",
      "sequential/dense/Relu                    ReLU            undef           NOT_RUN         0         \n",
      "sequential/dense_1/BiasAdd/Add           FullyConnected  jit_gemm_FP32   EXECUTED        42        \n",
      "sequential/max_pooling2d/MaxPool         Pooling         jit_avx512_FP32 EXECUTED        937       \n",
      "sequential/max_pooling2d_1/MaxPool       Pooling         jit_avx512_FP32 EXECUTED        650       \n"
     ]
    }
   ],
   "source": [
    "# retrieve performance counters from last inference request\n",
    "perf_counts = exec_net.requests[0].get_perf_counts()\n",
    "\n",
    "# display performance counters for each layer\n",
    "print(\"Performance counters:\")\n",
    "print(\"{:<40} {:<15} {:<15} {:<15} {:<10}\".format('name', 'layer_type', \n",
    "        'exec_type', 'status', 'real_time, us'))\n",
    "for layer, stats in perf_counts.items():\n",
    "    print(\"{:<40} {:<15} {:<15} {:<15} {:<10}\".format(layer,\n",
    "        stats['layer_type'], stats['exec_type'],\n",
    "        stats['status'], stats['real_time']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Now that we are done running the sample, we clean up by deleting objects before exiting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del exec_net\n",
    "del net\n",
    "del ie\n",
    "\n",
    "print(\"Resource objects removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "- [More Jupyter Notebook Tutorials](https://devcloud.intel.com/edge/get_started/tutorials/) - additional sample application Jupyter* Notebook tutorials\n",
    "- [Jupyter* Notebook Samples](https://devcloud.intel.com/edge/advanced/sample_applications/) - sample applications\n",
    "- [Intel® Distribution of OpenVINO™ toolkit Main Page](https://software.intel.com/openvino-toolkit) - learn more about the tools and use of the Intel® Distribution of OpenVINO™ toolkit for implementing inference on the edge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this notebook\n",
    "\n",
    "For technical support, please see the [Intel® DevCloud Forums](https://software.intel.com/en-us/forums/intel-devcloud-for-edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=background-color:#0071C5;color:white;padding:0.5em;display:table-cell;width:100pc;vertical-align:middle>\n",
    "<img style=float:right src=\"https://devcloud.intel.com/edge/static/images/svg/IDZ_logo.svg\" alt=\"Intel DevCloud logo\" width=\"150px\"/>\n",
    "<a style=color:white>Intel® DevCloud for the Edge</a><br>   \n",
    "<a style=color:white href=\"#top\">Top of Page</a> | \n",
    "<a style=color:white href=\"https://devcloud.intel.com/edge/static/docs/terms/Intel-DevCloud-for-the-Edge-Usage-Agreement.pdf\">Usage Agreement (Intel)</a> | \n",
    "<a style=color:white href=\"https://devcloud.intel.com/edge/static/docs/terms/Colfax_Cloud_Service_Terms_v1.3.pdf\">Service Terms (Colfax)</a>\n",
    "</p>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (OpenVINO)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "186.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
